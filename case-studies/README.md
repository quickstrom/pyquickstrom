Artifact Documentation
======================

We'll use the submitted artifact to reproduce the case study test results from "Quickstrom: Property-based acceptance testing with LTL specifications". It's
a Docker image that checks all (or only the specified) implementations of TodoMVC.

Getting Started Guide
---------------------

Requirements:

- Docker (preferrably installed directly on a Linux machine to avoid networking issues)
- A web browser to view test reports with

Getting started steps:

1. Load the image into Docker:

   ```bash
   docker load < case-study.tar.gz
   ```

2. Check a single TodoMVC application:

   ```bash
   docker run --network=host -v /tmp/case-study:/tmp/case-study case-study:firefox run-case-study backbone
   ```

   This will take about 1 minute. When finished, you should see some output
   ending with "Passed!".

3. Open `/tmp/case-study/backbone.firefox.1/html-report/index.html`. This is the
   report for the check, used for troubleshooting.

That's it, you're up and running!

## Step-by-Step Instructions

> The Step by Step Instructions explain how to reproduce any experiments or other
> activities that support the conclusions in your paper. Write this for readers
> who have a deep interest in your work and are studying it to improve it or
> compare against it. If your artifact runs for more than a few minutes, point
> this out and explain how to run it on smaller inputs.
>
> Where appropriate, include descriptions of and links to files (included in the
> archive) that represent expected outputs (e.g., the log files expected to be
> generated by your tool on the given inputs); if there are warnings that are safe
> to be ignored, explain which ones they are.
>
> The artifactâ€™s documentation should include the following:
>
> A list of claims from the paper supported by the artifact, and how/why.
>
> A list of claims from the paper not supported by the artifact, and how/why.
>
> Example:
> Performance claims cannot be reproduced in VM, authors are not allowed to
> redistribute specific benchmarks, etc. Artifact reviewers can then center their
> reviews / evaluation around these specific claims.

The case study runner has built-in expectations on the results of each
implemetation under test (see `case-studies/run.py` in the source for the full listing). An
expectation is either "passed", "failed", or "error". The runner checks all
implemetations sequentially and verifies that they match their expected results.

Because an implementation can pass unexpectedly, which is likely with the most
intricate bugs that a check might not uncover, the runner retries up to 5 times.
An unexpected failure or error causes the checker to continue.

Implementations that match the expected result are logged as:

```
Got expected RESULT!
```

while unexpected results are logged as:

```
Expected RESULT but result was RESULT!
```

At the end of the run, the runner lists the applications for which we didn't get
matching results, so that we can easily recheck only those.